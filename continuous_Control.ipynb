{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "from model import Actor, Critic\n",
    "from agent import Agent\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name='./Reacher_Linux20/Reacher.x86_64')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n",
      "(33,) (33,)\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents, env_info.vector_observations.shape[0])\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "print(states[0].shape, states[0,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actor and Critic Networks\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.elu(self.fc1(state))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.elu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4 #3e-5 #1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4 #3e-5 #1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY_actor = 0.0 #3e-4 #0        # L2 weight decay\n",
    "WEIGHT_DECAY_critic = 0.0 #1e-6 #0        # L2 weight decay\n",
    "\n",
    "#to decay exploration as it learns\n",
    "EPS_START=1.0\n",
    "EPS_END=0.05\n",
    "EPS_DECAY=3e-5\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR, weight_decay=WEIGHT_DECAY_actor)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY_critic)\n",
    "\n",
    "        # Noise process\n",
    "        #self.noise = OUNoise(action_size, random_seed) #single agent only\n",
    "        self.noise = OUNoise((num_agents, action_size), random_seed) #both singe and multiple agent\n",
    "        self.eps = EPS_START\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "        # Make sure target is initialized with the same weight as the source (found on slack to make big difference)\n",
    "        self.hard_update(self.actor_target, self.actor_local)\n",
    "        self.hard_update(self.critic_target, self.critic_local)\n",
    "\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        #Experience from each agent is separately saved (so it works for single or multi agent environment)\n",
    "        #This works because each agent is operating in a separate/independent environment\n",
    "        for a in range(self.num_agents):\n",
    "            self.memory.add(states[a], actions[a], rewards[a], next_states[a], dones[a])\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        #add noise according to epsilon probability\n",
    "        if add_noise and (np.random.random() < self.eps):\n",
    "            actions += self.noise.sample()\n",
    "            #update the exploration parameter\n",
    "            self.eps -= EPS_DECAY\n",
    "            if self.eps < EPS_END:\n",
    "                self.eps = EPS_END\n",
    "            #self.noise.reset() #not sure if need to do this here\n",
    "\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1.0) #clip the gradient for the critic network (Udacity hint)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "    ## from slack - Since you're using DDPG, @gregoriomezquita mentioned that \n",
    "    ## initializing the weights of the target networks to be the same as those \n",
    "    ## of the live networks seemed to make a huge difference\n",
    "    def hard_update(self, target, source):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(source_param.data)\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        #dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(state_size=env_info.vector_observations.shape[1], action_size=brain.vector_action_space_size, \n",
    " \n",
    "              num_agents=env_info.vector_observations.shape[0],  random_seed=0)\n",
    "#Agent\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4 #3e-5 #1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4 #3e-5 #1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY_actor = 0.0 #3e-4 #0        # L2 weight decay\n",
    "WEIGHT_DECAY_critic = 0.0 #1e-6 #0        # L2 weight decay\n",
    "\n",
    "#to decay exploration as it learns\n",
    "EPS_START=1.0\n",
    "EPS_END=0.05\n",
    "EPS_DECAY=3e-5\n",
    "\n",
    "\n",
    "\n",
    "#for single and multiple agents\n",
    "def ddpg(n_episodes=2000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_list = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment\n",
    "        states = env_info.vector_observations                   # get the current states (for all agents)\n",
    "        agent.reset() #reset the agent OU Noise\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get rewards (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += rewards                                  # update the score (for each agent)\n",
    "            agent.step(states, actions, rewards, next_states, dones) #train the agent         \n",
    "            # Extra Learning per time step (since generating so much experience at each step)\n",
    "            if len(agent.memory) > BATCH_SIZE:\n",
    "                for _ in range(3):\n",
    "                    experiences = agent.memory.sample()\n",
    "                    agent.learn(experiences, GAMMA)\n",
    "                    \n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        \n",
    "        scores_deque.append(np.mean(scores))\n",
    "        scores_list.append(np.mean(scores))\n",
    "        \n",
    "        #print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        print('Episode {}\\tAverage Score: {:.2f}\\tScore: {}'.format(i_episode, np.mean(scores_deque), np.mean(scores)))\n",
    "        print('Epsilon: {} and Memory size: {}'.format(agent.eps, len(agent.memory)))\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "        if np.mean(scores_deque) > 30 and len(scores_deque) >= 100:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            break\n",
    "            \n",
    "    return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surajit/Desktop/deep-reinforcement-learning/control_ball/agent.py:128: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1.0) #clip the gradient for the critic network (Udacity hint)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.57\tScore: 0.5674999873153865\n",
      "Epsilon: 0.970540000000025 and Memory size: 20020\n",
      "Episode 2\tAverage Score: 0.67\tScore: 0.7699999827891588\n",
      "Epsilon: 0.9419200000000494 and Memory size: 40040\n",
      "Episode 3\tAverage Score: 0.78\tScore: 0.995499977748841\n",
      "Epsilon: 0.9137800000000733 and Memory size: 60060\n",
      "Episode 4\tAverage Score: 1.04\tScore: 1.8249999592080712\n",
      "Epsilon: 0.8869000000000962 and Memory size: 80080\n",
      "Episode 5\tAverage Score: 1.28\tScore: 2.2554999495856465\n",
      "Epsilon: 0.8610400000001182 and Memory size: 100000\n",
      "Episode 6\tAverage Score: 1.49\tScore: 2.5349999433383346\n",
      "Epsilon: 0.8358700000001396 and Memory size: 100000\n",
      "Episode 7\tAverage Score: 1.68\tScore: 2.7854999377392233\n",
      "Epsilon: 0.8113600000001604 and Memory size: 100000\n",
      "Episode 8\tAverage Score: 1.87\tScore: 3.207499928306788\n",
      "Epsilon: 0.7880200000001802 and Memory size: 100000\n",
      "Episode 9\tAverage Score: 2.10\tScore: 3.9674999113194644\n",
      "Epsilon: 0.7645000000002002 and Memory size: 100000\n",
      "Episode 10\tAverage Score: 2.38\tScore: 4.895999890565872\n",
      "Epsilon: 0.7413700000002199 and Memory size: 100000\n",
      "Episode 11\tAverage Score: 2.72\tScore: 6.147499862592667\n",
      "Epsilon: 0.7196500000002384 and Memory size: 100000\n",
      "Episode 12\tAverage Score: 2.98\tScore: 5.819499869924039\n",
      "Epsilon: 0.6983200000002565 and Memory size: 100000\n",
      "Episode 13\tAverage Score: 3.29\tScore: 6.941499844845384\n",
      "Epsilon: 0.6776500000002741 and Memory size: 100000\n",
      "Episode 14\tAverage Score: 3.74\tScore: 9.588499785680323\n",
      "Epsilon: 0.6574000000002913 and Memory size: 100000\n",
      "Episode 15\tAverage Score: 4.10\tScore: 9.140499795693904\n",
      "Epsilon: 0.6387400000003072 and Memory size: 100000\n",
      "Episode 16\tAverage Score: 4.44\tScore: 9.575499785970896\n",
      "Epsilon: 0.6199900000003231 and Memory size: 100000\n",
      "Episode 17\tAverage Score: 4.70\tScore: 8.960999799706041\n",
      "Epsilon: 0.6016900000003387 and Memory size: 100000\n",
      "Episode 18\tAverage Score: 5.18\tScore: 13.24249970400706\n",
      "Epsilon: 0.5828800000003547 and Memory size: 100000\n",
      "Episode 19\tAverage Score: 5.61\tScore: 13.451499699335546\n",
      "Epsilon: 0.5656600000003693 and Memory size: 100000\n",
      "Episode 20\tAverage Score: 6.32\tScore: 19.790999557636678\n",
      "Epsilon: 0.5492200000003833 and Memory size: 100000\n",
      "Episode 21\tAverage Score: 7.10\tScore: 22.574499495420604\n",
      "Epsilon: 0.533080000000397 and Memory size: 100000\n",
      "Episode 22\tAverage Score: 7.94\tScore: 25.705999425426125\n",
      "Epsilon: 0.5172100000004105 and Memory size: 100000\n",
      "Episode 23\tAverage Score: 8.94\tScore: 30.813499311264604\n",
      "Epsilon: 0.5021500000004233 and Memory size: 100000\n",
      "Episode 24\tAverage Score: 10.02\tScore: 34.91999921947718\n",
      "Epsilon: 0.4881700000004352 and Memory size: 100000\n",
      "Episode 25\tAverage Score: 11.11\tScore: 37.25099916737527\n",
      "Epsilon: 0.4733800000004478 and Memory size: 100000\n",
      "Episode 26\tAverage Score: 12.13\tScore: 37.77649915562942\n",
      "Epsilon: 0.45901000000046 and Memory size: 100000\n",
      "Episode 27\tAverage Score: 13.13\tScore: 39.02299912776798\n",
      "Epsilon: 0.44482000000047206 and Memory size: 100000\n",
      "Episode 28\tAverage Score: 14.02\tScore: 38.050999149493876\n",
      "Epsilon: 0.43192000000048303 and Memory size: 100000\n",
      "Episode 29\tAverage Score: 14.88\tScore: 38.94799912944436\n",
      "Epsilon: 0.4193800000004937 and Memory size: 100000\n",
      "Episode 30\tAverage Score: 15.68\tScore: 38.81349913245067\n",
      "Epsilon: 0.40675000000050443 and Memory size: 100000\n",
      "Episode 31\tAverage Score: 16.41\tScore: 38.435999140888455\n",
      "Epsilon: 0.39412000000051517 and Memory size: 100000\n",
      "Episode 32\tAverage Score: 17.09\tScore: 38.18349914653227\n",
      "Epsilon: 0.3832600000005244 and Memory size: 100000\n",
      "Episode 33\tAverage Score: 17.72\tScore: 37.693499157484624\n",
      "Epsilon: 0.3715300000005344 and Memory size: 100000\n",
      "Episode 34\tAverage Score: 18.34\tScore: 38.83999913185835\n",
      "Epsilon: 0.36052000000054374 and Memory size: 100000\n",
      "Episode 35\tAverage Score: 18.92\tScore: 38.605499137099834\n",
      "Epsilon: 0.3494800000005531 and Memory size: 100000\n",
      "Episode 36\tAverage Score: 19.46\tScore: 38.41199914142489\n",
      "Epsilon: 0.3385600000005624 and Memory size: 100000\n",
      "Episode 37\tAverage Score: 19.95\tScore: 37.761999155953525\n",
      "Epsilon: 0.3280000000005714 and Memory size: 100000\n",
      "Episode 38\tAverage Score: 20.43\tScore: 38.07799914889038\n",
      "Epsilon: 0.31924000000057884 and Memory size: 100000\n",
      "Episode 39\tAverage Score: 20.90\tScore: 38.79649913283065\n",
      "Epsilon: 0.3094000000005872 and Memory size: 100000\n",
      "Episode 40\tAverage Score: 21.35\tScore: 39.050999127142134\n",
      "Epsilon: 0.3003700000005949 and Memory size: 100000\n",
      "Episode 41\tAverage Score: 21.78\tScore: 38.88799913078547\n",
      "Epsilon: 0.2917900000006022 and Memory size: 100000\n",
      "Episode 42\tAverage Score: 22.19\tScore: 39.08499912638217\n",
      "Epsilon: 0.28324000000060945 and Memory size: 100000\n",
      "Episode 43\tAverage Score: 22.57\tScore: 38.245999145135286\n",
      "Epsilon: 0.2746900000006167 and Memory size: 100000\n",
      "Episode 44\tAverage Score: 22.93\tScore: 38.56049913810566\n",
      "Epsilon: 0.26713000000062315 and Memory size: 100000\n",
      "Episode 45\tAverage Score: 23.23\tScore: 36.20349919078872\n",
      "Epsilon: 0.2587000000006303 and Memory size: 100000\n",
      "Episode 46\tAverage Score: 23.52\tScore: 36.77599917799235\n",
      "Epsilon: 0.2513200000006366 and Memory size: 100000\n",
      "Episode 47\tAverage Score: 23.75\tScore: 34.22749923495576\n",
      "Epsilon: 0.24400000000063726 and Memory size: 100000\n",
      "Episode 48\tAverage Score: 24.01\tScore: 36.230999190174046\n",
      "Epsilon: 0.2364700000006367 and Memory size: 100000\n",
      "Episode 49\tAverage Score: 24.25\tScore: 35.63849920341745\n",
      "Epsilon: 0.22933000000063616 and Memory size: 100000\n",
      "Episode 50\tAverage Score: 24.54\tScore: 38.71899913456291\n",
      "Epsilon: 0.22234000000063564 and Memory size: 100000\n",
      "Episode 51\tAverage Score: 24.79\tScore: 37.31199916601181\n",
      "Epsilon: 0.21550000000063513 and Memory size: 100000\n",
      "Episode 52\tAverage Score: 25.03\tScore: 37.39199916422367\n",
      "Epsilon: 0.20935000000063467 and Memory size: 100000\n",
      "Episode 53\tAverage Score: 25.27\tScore: 37.98049915106967\n",
      "Epsilon: 0.2030500000006342 and Memory size: 100000\n",
      "Episode 54\tAverage Score: 25.50\tScore: 37.67249915795401\n",
      "Epsilon: 0.19711000000063375 and Memory size: 100000\n",
      "Episode 55\tAverage Score: 25.72\tScore: 37.41249916376546\n",
      "Epsilon: 0.1911400000006333 and Memory size: 100000\n",
      "Episode 56\tAverage Score: 25.93\tScore: 37.741499156411734\n",
      "Epsilon: 0.18541000000063287 and Memory size: 100000\n",
      "Episode 57\tAverage Score: 26.14\tScore: 37.898999152891335\n",
      "Epsilon: 0.17977000000063245 and Memory size: 100000\n",
      "Episode 58\tAverage Score: 26.30\tScore: 35.34599920995534\n",
      "Epsilon: 0.17446000000063205 and Memory size: 100000\n",
      "Episode 59\tAverage Score: 26.47\tScore: 36.30399918854236\n",
      "Epsilon: 0.16906000000063165 and Memory size: 100000\n",
      "Episode 60\tAverage Score: 26.60\tScore: 34.40049923108891\n",
      "Epsilon: 0.1644100000006313 and Memory size: 100000\n",
      "Episode 61\tAverage Score: 26.71\tScore: 33.19399925805628\n",
      "Epsilon: 0.15949000000063093 and Memory size: 100000\n",
      "Episode 62\tAverage Score: 26.86\tScore: 35.67299920264632\n",
      "Epsilon: 0.15445000000063056 and Memory size: 100000\n",
      "Episode 63\tAverage Score: 27.01\tScore: 36.632999181188644\n",
      "Epsilon: 0.14995000000063022 and Memory size: 100000\n",
      "Episode 64\tAverage Score: 27.17\tScore: 37.21599916815758\n",
      "Epsilon: 0.1455700000006299 and Memory size: 100000\n",
      "Episode 65\tAverage Score: 27.33\tScore: 37.71649915697053\n",
      "Epsilon: 0.14125000000062957 and Memory size: 100000\n",
      "Episode 66\tAverage Score: 27.45\tScore: 34.88699922021478\n",
      "Epsilon: 0.13741000000062928 and Memory size: 100000\n",
      "Episode 67\tAverage Score: 27.55\tScore: 34.371499231737104\n",
      "Epsilon: 0.13330000000062897 and Memory size: 100000\n",
      "Episode 68\tAverage Score: 27.64\tScore: 33.2854992560111\n",
      "Epsilon: 0.12934000000062867 and Memory size: 100000\n",
      "Episode 69\tAverage Score: 27.77\tScore: 36.72499917913228\n",
      "Epsilon: 0.1257400000006284 and Memory size: 100000\n",
      "Episode 70\tAverage Score: 27.92\tScore: 38.71699913460761\n",
      "Epsilon: 0.12217000000062814 and Memory size: 100000\n",
      "Episode 71\tAverage Score: 28.07\tScore: 38.6379991363734\n",
      "Epsilon: 0.11851000000062786 and Memory size: 100000\n",
      "Episode 72\tAverage Score: 28.22\tScore: 38.21349914586172\n",
      "Epsilon: 0.11425000000062754 and Memory size: 100000\n",
      "Episode 73\tAverage Score: 28.33\tScore: 36.40849918620661\n",
      "Epsilon: 0.11089000000062729 and Memory size: 100000\n",
      "Episode 74\tAverage Score: 28.45\tScore: 37.19199916869402\n",
      "Epsilon: 0.10729000000062702 and Memory size: 100000\n",
      "Episode 75\tAverage Score: 28.56\tScore: 37.15199916958809\n",
      "Epsilon: 0.10390000000062677 and Memory size: 100000\n",
      "Episode 76\tAverage Score: 28.64\tScore: 34.27649923386052\n",
      "Epsilon: 0.10129000000062657 and Memory size: 100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 77\tAverage Score: 28.76\tScore: 38.203499146085235\n",
      "Epsilon: 0.09856000000062637 and Memory size: 100000\n",
      "Episode 78\tAverage Score: 28.87\tScore: 37.086499171052125\n",
      "Epsilon: 0.09541000000062613 and Memory size: 100000\n",
      "Episode 79\tAverage Score: 29.00\tScore: 38.91299913022667\n",
      "Epsilon: 0.09277000000062594 and Memory size: 100000\n",
      "Episode 80\tAverage Score: 29.12\tScore: 38.85199913159013\n",
      "Epsilon: 0.09034000000062575 and Memory size: 100000\n",
      "Episode 81\tAverage Score: 29.24\tScore: 38.56899913791567\n",
      "Epsilon: 0.08788000000062557 and Memory size: 100000\n",
      "Episode 82\tAverage Score: 29.35\tScore: 38.672499135602266\n",
      "Epsilon: 0.08494000000062535 and Memory size: 100000\n",
      "Episode 83\tAverage Score: 29.45\tScore: 37.79599915519357\n",
      "Epsilon: 0.08257000000062517 and Memory size: 100000\n",
      "Episode 84\tAverage Score: 29.55\tScore: 37.770999155752364\n",
      "Epsilon: 0.08008000000062498 and Memory size: 100000\n",
      "Episode 85\tAverage Score: 29.64\tScore: 36.81949917702004\n",
      "Epsilon: 0.07795000000062483 and Memory size: 100000\n",
      "Episode 86\tAverage Score: 29.73\tScore: 37.42649916345253\n",
      "Epsilon: 0.07591000000062467 and Memory size: 100000\n",
      "Episode 87\tAverage Score: 29.81\tScore: 36.85349917626009\n",
      "Epsilon: 0.07387000000062452 and Memory size: 100000\n",
      "Episode 88\tAverage Score: 29.89\tScore: 37.05299917180091\n",
      "Epsilon: 0.07207000000062438 and Memory size: 100000\n",
      "Episode 89\tAverage Score: 29.99\tScore: 38.19349914630875\n",
      "Epsilon: 0.07018000000062424 and Memory size: 100000\n",
      "Episode 90\tAverage Score: 30.03\tScore: 34.356999232061206\n",
      "Epsilon: 0.06787000000062407 and Memory size: 100000\n",
      "Episode 91\tAverage Score: 30.12\tScore: 38.06449914919212\n",
      "Epsilon: 0.06580000000062391 and Memory size: 100000\n",
      "Episode 92\tAverage Score: 30.20\tScore: 37.59899915959686\n",
      "Epsilon: 0.06376000000062376 and Memory size: 100000\n",
      "Episode 93\tAverage Score: 30.29\tScore: 37.92049915241078\n",
      "Epsilon: 0.06190000000062362 and Memory size: 100000\n",
      "Episode 94\tAverage Score: 30.37\tScore: 38.04649914959445\n",
      "Epsilon: 0.060040000000623484 and Memory size: 100000\n",
      "Episode 95\tAverage Score: 30.43\tScore: 35.888999197818336\n",
      "Epsilon: 0.05800000000062333 and Memory size: 100000\n",
      "Episode 96\tAverage Score: 30.48\tScore: 35.709499201830475\n",
      "Epsilon: 0.05650000000062322 and Memory size: 100000\n",
      "Episode 97\tAverage Score: 30.55\tScore: 36.616499181557444\n",
      "Epsilon: 0.05503000000062311 and Memory size: 100000\n",
      "Episode 98\tAverage Score: 30.60\tScore: 35.383999209105966\n",
      "Epsilon: 0.053530000000622996 and Memory size: 100000\n",
      "Episode 99\tAverage Score: 30.66\tScore: 36.56849918263033\n",
      "Epsilon: 0.052030000000622884 and Memory size: 100000\n",
      "Episode 100\tAverage Score: 30.66\tScore: 31.0599993057549\n",
      "Epsilon: 0.050590000000622776 and Memory size: 100000\n",
      "Episode 100\tAverage Score: 30.66\n",
      "Episode 100\tAverage Score: 30.66\n"
     ]
    }
   ],
   "source": [
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scores_file.json\", \"w\") as write_file:\n",
    "        json.dump(scores, write_file)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "agent = Agent(state_size=env_info.vector_observations.shape[1], action_size=brain.vector_action_space_size, \n",
    "              num_agents=env_info.vector_observations.shape[0],  random_seed=0)\n",
    "\n",
    "#Since the model is trained on gpu, need to load all gpu tensors to cpu:\n",
    "\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth', map_location=lambda storage, loc: storage))\n",
    "agent.eps = 0.05\n",
    "scores_list = []\n",
    "def ddpg_inference(n_episodes=50):\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment\n",
    "        states = env_info.vector_observations                   # get the current states (for all agents)\n",
    "        agent.reset() #reset the agent OU Noise\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get rewards (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += rewards                                  # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print('Episode {}: {}'.format(i_episode, np.mean(scores)))\n",
    "        scores_list.append(np.mean(scores))\n",
    "    print('Mean score is: ', np.mean(np.array(scores_list)))\n",
    "    \n",
    "ddpg_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
